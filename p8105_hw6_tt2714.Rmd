---
title: "p8105_hw6_tt2714"
author: "Tiffany Tu"
date: "11/19/2018"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(readr)
library(dplyr)
library(tidyr)
library(tidyverse)
library(knitr)
library(MASS)
library(forcats)
library(broom)
library(modelr)
library(ggplot2)
```

## Problem 1
#### Data cleaning...
A city_state variable is created to combine the two columns city and state. Four cities that do not report victim race are removed. Disposition, victim_race, and victim_sex columns are coded into binary variables.      
```{r message = FALSE, warning = FALSE}
homicide = read_csv(file = "./homicide-data.csv") 
homicide_citystate = homicide %>% 
  unite(city_state, c(city, state), sep = ", ", remove = T) %>% 
  filter(!grepl('Dallas|Phoenix|Kansas City|Tulsa', city_state)) %>% 
  mutate(disposition = ifelse(disposition == "Closed by arrest", 1, 0),
         victim_race = ifelse(victim_race == "White", "white", "non-white")) %>% 
  mutate(victim_age = as.numeric(victim_age), 
         victim_sex = as.factor(victim_sex),
         victim_race = fct_relevel(victim_race, "white")) 
```

#### Logistic Regression Model for Baltimore, MD 
Outcome: disposition of homicide    
Reference group: race - white        
Predictors: sex and age    

Use `broom::tidy` to clean up the model and include odds ratio with confidence intervals. 
```{r}
baltimore = homicide_citystate %>% filter(city_state == "Baltimore, MD")
baltimore_glm = glm(disposition ~ victim_race + victim_age + victim_sex, 
                    data = baltimore, family = binomial())

baltimore_glm %>% 
  broom::tidy(., conf.int = TRUE) %>% 
  mutate(odds.ratio = exp(estimate),
         conf.low = exp(conf.low),
         conf.high = exp(conf.high)) %>% 
  kable(digits = 3)
```

#### Regression on all cities
Logistic regression on all cities with odds ratio and confidence intervals.
```{r}
cities_glm = homicide_citystate %>% 
  group_by(city_state) %>% 
  nest() %>% 
  mutate(models = map(data, ~glm(disposition ~ victim_race + victim_age + victim_sex, data = .x)),
         models = map(models, broom::tidy)) %>% 
  dplyr::select(-data) %>% 
  unnest() %>% 
  mutate(odds.ratio = exp(estimate),
         conf.low = exp(estimate - std.error * 1.96),
         conf.high = exp(estimate + std.error * 1.96)) %>% 
  filter(term == "victim_racenon-white")
```

```{r}
cities_glm %>% ggplot(aes(color = reorder(city_state, odds.ratio))) +
  geom_point(aes(x = reorder(city_state, odds.ratio), y = odds.ratio)) +
  geom_errorbar(aes(x = city_state, ymin = conf.low, ymax = conf.high)) +
  coord_flip() + ggtitle("Adjusted odds ratio of non-white to white in each city") +
  xlab("city") + ylab("adjusted odds ratio") + 
  theme(axis.text.y = element_text(size = 6), legend.position = "none") 
```

Cities `r (cities_glm %>% filter(odds.ratio >1))$city_state` show an odds ratio greater than 1, meaning that more homicides are solved for non-white than whites. All other cities have odds ratio less than 1, with more homicides solved for whites than non-whites. The confidence intervals on average decreases as the odds ratio decreases, as we see that the top three cities CIs gives a wider range.  

## Problem 2
```{r message = FALSE, warning = FALSE}
birthweight = read_csv(file = "./birthweight.csv")
birthweight_new = birthweight %>% 
  mutate(babysex = as.factor(babysex),
         frace = as.factor(frace),
         mrace = as.factor(mrace),
         malform = as.factor(malform))
```
This dataset consists of `r ncol(birthweight)` variables on a child's birthweight. There is a total of `r nrow(birthweight)` entries of children data. The data is prepared for regression analysis with appropriate format and no missing data. 

#### Regression Model for Birthweight
I am using a step-wise regression (backward direction) for variable selection, as we have `r ncol(birthweight_new) - 1` variables available for birthweight prediction. 
```{r results = FALSE}
model_1 = lm(bwt ~ ., data = birthweight_new)
stepwise_1 = step(model_1, direction = 'backward') %>% 
  broom::tidy()
```

```{r}
stepwise_1 %>% kable()
```

This results in `r nrow(stepwise_1) - 1` correlated predictors for birthweight. But because this is still a lot of predictors to be considered, I select the top four with highest coefficients for my model: `babysex`, `blength`, `parity`, and `bhead`.

```{r}
model_1regression = lm(bwt ~ babysex + blength + parity + bhead, data = birthweight_new)
```

```{r}
birthweight_new %>% 
  add_predictions(., model_1regression) %>% 
  add_residuals(., model_1regression) %>% 
  ggplot(., aes(x = pred, y = resid)) +
  geom_point(alpha = 0.15) +
  xlab("Predicted Values") + ylab("Residuals") + 
  ggtitle("Residual against fitted values for birthweight prediction")
```
From the plot we see a heavy cluster from weight 2500 to 3500 around the zero mark for residual. However, there remains several outliers below the weight of 1500.

#### Additional Models
Now we'll considered two other models and compare our results using cross validation. 
   
Model_2 predictors: length at birth and gestational age
```{r}
model_2regression = lm(bwt ~ blength + gaweeks, 
                       data = birthweight_new)
```
Model_3 predictors: head circumference, length, sex and all interactions
```{r}
model_3regression = lm(bwt ~ bhead + blength + babysex + 
                         bhead * blength + bhead * babysex + 
                         blength * babysex + 
                         bhead * blength * babysex, 
                       data = birthweight_new)
```

#### Comparing Models
Cross validation using `modelr`. First split into testing and training datasets. 
```{r}
cv_birthweight = 
  crossv_mc(birthweight_new, 1000)
```

Obtain RMSEs for testing data
```{r warning = FALSE}
cv_birthweight = cv_birthweight %>% 
  mutate(model_1 = map(train, ~lm(bwt ~ babysex + blength + 
                                   parity + bhead, data = .x)), 
         model_2 = map(train, ~lm(bwt ~ blength + gaweeks, data = .x)),
         model_3 = map(train, ~lm(bwt ~ bhead + blength + babysex + bhead * blength + 
                                   bhead * babysex + blength * babysex + 
                                   bhead * blength * babysex, data = .x))) %>% 
  mutate(rmse_1 = map2_dbl(model_1, test, ~rmse(model = .x, data = .y)), 
         rmse_2 = map2_dbl(model_2, test, ~rmse(model = .x, data = .y)),
         rmse_3 = map2_dbl(model_3, test, ~rmse(model = .x, data = .y))) 
```

Plot prediction error distribution
```{r}
cv_birthweight %>% 
  dplyr::select(starts_with("rmse")) %>% 
  gather(key = model, value = rmse) %>% 
  mutate(model = str_replace(model, "rmse_", ""),
         model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse, fill = model)) + geom_violin() + 
  ggtitle("RMSE Comparison with Cross Validation")
```

From the violin plot, we see that my model seems to be doing similarily as model_3, the model with all interactions. Model_2, the model with length at birth and gestational age shows high RMSE distribution. We can say that the stepwise regression for variable selection was helpful, but interaction as well as outliers must be considered for a more rigorous model. 